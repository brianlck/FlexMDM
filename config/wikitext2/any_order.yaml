trainer: "any-order-flow"
dataset: "wikitext2"
model:
  hidden_size: 256
  n_heads: 4
  cond_dim: 64
  dropout: 0.1
  n_blocks: 4
interpolant:
  type: "any-order"
  tokens: null # filled in automatically
  pad_token: null # filled in automatically
  mask_token: null # filled in automatically
  max_length: 512
  insert_schedule:
    type: "geometric"
    min: 5
    max: 0.01
  unmask_schedule:
    type: "linear"
training:
  batch_size: 32
  learning_rate: 0.0001
  devices: 4
  num_epochs: 200
  checkpoint_dir: "checkpoints/wikitext2"
  save_top_k: 1
  save_every_n_epochs: 1
  loss_fn:
    unmask: "elbo"
    insert: "expectation"
wandb:
  entity: "jaeyeon_kim-harvard-university"
  project: "interpretable-flow"
  name: "wikitext2-any-order"
  