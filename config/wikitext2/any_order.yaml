dataset: "wikitext2"
model:
  hidden_size: 256
  n_heads: 4
  cond_dim: 64
  dropout: 0.1
  n_blocks: 4
interpolant:
  type: "any-order"
  max_length: 1024
  mask_schedule:
    min: 5
    max: 0.01
training:
  batch_size: 32
  learning_rate: 0.0001
  devices: 4
  num_epochs: 100
  checkpoint_dir: "checkpoints/semi-auto"
